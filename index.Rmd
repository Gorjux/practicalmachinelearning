---
title: "practical Machine Learning Course Project"
author: "Barbara Gorjux"
date: "27 mai 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### **INTRODUCTION:**
The goal of this project is to predict the manner in which the participants did the exercise. We use the dataset Weight Lifting Exercises (WLE). In this data set, the «classe» variable quatify "how well" the participants did the exercise. 

I will fit two models and choice one of then to make the prediction in the testing set.

URL and info about the data set:
 http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har)
This dataset collected by Velloso, E. ; Bulling, A. ; Gellersen, H. ; Ugulino, W. ; Fuks, H.. Thanks for their generosity.

#### **Loading library**
```{r}
library(lattice)
library(ggplot2)
library(caret)

```
### **DATA PREPARATION**
#### **Loading Data sets**
```{r}
destfile="pml-training.csv"
fileURL<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
if(!file.exists(destfile)){
  download.file(fileURL, destfile, method="auto")}

destfile="pml-testing.csv"
fileURL<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!file.exists(destfile)){
  download.file(fileURL, destfile, method="auto")}
```
#### **Reading the data sets**
```{r}
train<-read.csv(file = "pml-training.csv")
test<-read.csv(file = "pml-testing.csv")

```
#### **Cleaning the data**

* **First step**: remove the variables which contain fast only NA's values 
```{r}
onlyNA<-sapply(train, function(x) mean(is.na(x)))>0.95
training<-train[, onlyNA==F]

```

* **Second step**: remove the 7 first variables which don't measure how the participant performe the exercise
```{r}
training<-training[,-(1:7)]
```

* **Third step**: remove the variables which have a near zero variability
```{r}
nzv<-nearZeroVar(training)
training<-training[,-nzv]
```

* **Fourth step**: use PCA (principal component analysis) to choice the more relevant variables
```{r}
training.matrix<-data.matrix(training, rownames.force=NA) #convert the data frame into a matrix
dim(training.matrix)
pca<-prcomp(training.matrix, scale=TRUE) #calculate PCA
pca.var<-pca$sde^2 #variation of each component
pca.var.per<-round(pca.var/sum(pca.var)*100,3)#percentage of variation
pca.var.per 
sum(pca.var.per[1:40])#the first 40 components account for 99% of the variation
variable_score<-pca$rotation[,1]# load the rotation of each component
va_variable_score<-abs(variable_score)#take the absolute value
variable_score_ranked<-sort(va_variable_score, decreasing = TRUE)#rank the component
top_40_variables<-names(variable_score_ranked[1:40])#extract the names of the 40 best variables

training<-training[top_40_variables]
```
* **fifth step**: cheaking if 'classe' variable is allways in the data
```{r}
"classe"%in%names(training)
```
#### **Splitting the training data into a training and a testing data set**
```{r}
inTrain<-createDataPartition(y=training$classe, p=0.7, list=F)
training<-training[inTrain,]
testing<-training[-inTrain,]

```
#### **For reproductibility**
```{r}
set.seed(10)
```

### **FITTING TWO MODELS**

#### **3-fold cross validation**
```{r}
fitControl<-trainControl(method="cv",number=3, verboseIter = FALSE)
```

#### **First model: Random forest**
```{r}
fitRF<-train(classe~., data=training, method="rf", trControl=fitControl)
```

#### **Second model: Linear discriminant analysis**
```{r}
fitLDA<-train(classe~., data=training, method="lda", trControl=fitControl, verbose=FALSE)

```

### **COMPARING THE MODELS** 
```{r}
print(fitRF)
fitRF$finalModel
print(fitLDA)
#The Random forest model is better as the LDA, because he has a better accuracy and a better kappa. So I choice the random forest model.

```

### **PREDICTION WITH TRAINING**
```{r}
predTRAINING<-predict(fitRF, training)
confusionMatrix(training$classe, predTRAINING)
```

### **PREDICTION WITH TESTING**
```{r}
predTRAINING<-predict(fitRF, testing)
confusionMatrix(testing$classe, predTRAINING)

```



